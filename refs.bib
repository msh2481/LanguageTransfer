# Pretraining

@article{papadimitriou2020learning,
  title={Learning music helps you read: Using transfer to study linguistic structure in language models},
  author={Papadimitriou, Isabel and Jurafsky, Dan},
  journal={arXiv preprint arXiv:2004.14601},
  year={2020}
}
"
Introduce Test for Inductive Bias via Language Model Transfer (TILT).

Models:
- LSTM (AWD-LM, 3 layers, 300-dimensional word embeddings, hidden size 1150, dropout 0.65 for embeddings and 0.3 for other parameters, ASGD).

Approach:
- Pretraining, then finetuning only embeddings

Tasks:
- Flat. Sequence of integers, each twice, with the distribution of distances taken from some Spanish corpus.
- Nested. Same, but generated by stack-based grammar.
- And some natural datasets, such as music, Java code and languages.

Results:
- Synthetic tasks are better than random but worse than natural tasks.
- Nested and flat are the same.
"

@article{artetxe2019cross,
  title={On the cross-lingual transferability of monolingual representations},
  author={Artetxe, Mikel and Ruder, Sebastian and Yogatama, Dani},
  journal={arXiv preprint arXiv:1910.11856},
  year={2019}
}
"
Models:
- Multilingual BERT?

Approach:
- Pretraining on English, then finetuning only embeddings to L2, then finetuning in Engish with frozen original embeddings for the new task, and zero-shot testing on the task in L2

Tasks:
- XNLI
- XQuAD

Results:
- BERT can transfer to tasks in L2 using only general L2 texts and task supervision in L1
"

@article{ri2022pretraining,
  title={Pretraining with artificial language: Studying transferable knowledge in language models},
  author={Ri, Ryokan and Tsuruoka, Yoshimasa},
  journal={arXiv preprint arXiv:2203.10326},
  year={2022}
}
"
Models:
- LSTM: d_input=300, d_hidden=294
- Transformer: d_model=300, d_mlp=600, n_heads=4
- They try both CLM and MLM for Transformers
- 3 layers and 6.9M parameters for both.

Approach: Same as papadimitriou2020learning

Tasks:
- Log-linear. Each sentense has a discourse vector c and words in a sentence follow the distribution softmax(c * v).
- Flat, but with different tokens for openning and closing brackets. E.g. <1 <3 <2 3> 2> 1>
- Nesting, also with different tokens. E.g. <1 <2 2> 1>

Results:
- Transformer is more flexible
- Nesting is better, when tokens in pairs are different
"

@misc{papadimitriou2023injecting,
  title={Injecting structural hints: Using language models to study inductive biases in language learning}, 
  author={Isabel Papadimitriou and Dan Jurafsky},
  year={2023},
  eprint={2304.13060},
  archivePrefix={arXiv},
  primaryClass={cs.CL}
}
"
https://github.com/toizzy/injecting-structural-hints

Models:
- GPT-2-small, length and batch size 512, 10^9 tokens for each language. 5000 steps, including 1000 for warmup.

Approach:
- Pretrain on synthetic language, then finetune whole model on L2.
- L2 is either English, or Japanese, or Basque.

Tasks:
- Random
- Repeat (with fixed length)
- Flat and Nesting as in papadimitriou2020learning
- Also mixed ones

Results:
- Flat, even in small proportion, is better than nesting. It is also higher in Chomsky hierarchy.
"

@article{lu2021pretrained,
  title={Pretrained transformers as universal computation engines},
  author={Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
  journal={arXiv preprint arXiv:2103.05247},
  volume={1},
  year={2021}
}
"
Models:
- GPT-2-small

Approach:
- Take a pretrained GPT-2, then fine-tune positional, input and output embeddings, as well as affine layer norm parameters. It can actually explain most of the performance, because tuning only BatchNorms is known to be effective.

Pretraining tasks:
- No pretraining
- Natural language
- Bit memory
- ViT pretrained on ImageNet-21k  

Testing tasks:
- Bit Memory Task: This exercise assesses a model's recall capability. It involves memorizing and reconstructing 1000-bit long sequences. A model is shown five such bitstrings and later presented with a partially masked version of one string. With a 50% chance for each bit to be hidden, the model's challenge is to recreate the original sequence. The bitstrings are processed in chunks of 50, amounting to 120 segments each 50 bits wide.
- Bit XOR Task: This task gauges a model's ability to perform bitwise exclusive OR operations. Two 5-bit long strings are provided to the model, which must predict the XOR outcome for each corresponding pair of bits. The model processes the bits one at a time, handling a total of 10 single-dimensional tokens.
- ListOps: Originating from research by Tay et al., this task tests a model's proficiency in interpreting and evaluating nested list operations. Models encounter mathematical expressions and must determine the correct numerical result. The complexity lies in the sequence's length, with the model being fed one token at a time up to a total of 512 tokens, each 15-dimensional.
- MNIST Classification: This standard benchmark in machine learning involves classifying handwritten digits from a grid of 32x32 pixels. The images are divided into 4x4 patches, providing the model with 64 tokens, each 16-dimensional, to analyze and categorize into the correct digit.
- CIFAR-10 Benchmark: Similar to MNIST, this task involves image classification. However, the images are color and more complex, from the CIFAR-10 dataset. The processing method is akin to MNIST, with 4x4 image patches creating 64 tokens of 16 dimensions each.
- CIFAR-10 LRA: A variation of CIFAR-10 from the Long Range Arena benchmark, this task is more demanding due to the images being grayscaled and flattened. The model confronts a single-dimensional token sequence of 1024 elements, compelling it to discern patterns over a much longer sequence with limited spatial cues.
- Remote Homology Detection: This task is about predicting protein structures based on amino acid sequences. Using the TAPE datasets, the model must identify the correct protein fold from a selection of 1195 possible labels. It does not rely on pretraining common in other studies and focuses on sequences up to 1024 amino acids in length. Each sequence is represented by up to 1024 tokens, each 25-dimensional, corresponding to the variety of common and uncommon amino acids.

Results:
- Tuning only LayerNorm parameters and embeddings in Transformers is already quite effective. Random LSTMs with only tuned embeddings are much worse. 
- LayerNorm parameters are very important to fine-tune.
"

@inproceedings{chiang2022transferability,
  title={On the transferability of pre-trained language models: A study from artificial datasets},
  author={Chiang, Cheng-Han and Lee, Hung-yi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={10},
  pages={10518--10525},
  year={2022}
}
"
Models:
- RoBERTa-medium

Approach:
- Pretrain on synthethic L1, fine-tune and test on L2

Pretraining tasks:
- English
- Kannada
- No pretraining
- Uniform
- Uni-gram
- Bi-gram
- Flat
- Nesting
- Shuffle. Many blocks, in each consecutive integers are shuffled. To perform MLM on such tasks model needs to find what integer is missing among the unmasked neighbors.

Testing tasks:
- GLUE
"

# Testing 
@article{kharitonov2020they,
  title={What they do when in doubt: a study of inductive biases in seq2seq learners},
  author={Kharitonov, Eugene and Chaabouni, Rahma},
  journal={arXiv preprint arXiv:2006.14953},
  year={2020}
}
"
Tasks: 
- Count or memorization: (aaa, bbb) in train, (aa, bb or bbb) in test
- Memorize or add or multiply: (aa, bbbb) in train, (a, bb or bbb or bbb) in test
- Hierarchical or linear: (aabaa, b) in train, (aba, a or b) in test
- Composition or memorization: (a, a), (b, b), (thrice a, aaa) in train, (thrice b, bbb or b)
"
  
# Interpretability

@article{voita2019analyzing,
  title={Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  journal={arXiv preprint arXiv:1905.09418},
  year={2019}
}
"
Descsribes three types of attention heads: positional, syntactic, rare words. Perhaps rare words is a refined form of averaging, because you donâ€™t want to update much on seeing 'the' or some other common token, their distribution is quite stable.
"

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}
"
Discusses heads that do averaging from the previous tokens that can be the next one according to corpus-wise bigram statistics. Maybe they are the same as rare words heads, because in cases where the next word is probably rare, such heads will attend to this rare word, and in other cases they will attend more uniformly. Also, they point out that most of the heads in one-layer Transformer are copying.
"