{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from typing import Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from beartype import beartype as typed\n",
    "from beartype.door import die_if_unbearable as assert_type\n",
    "from datasets import load_dataset\n",
    "from einops import einops as ein\n",
    "from jaxtyping import Bool, Float, Int\n",
    "from torch import Tensor as TT\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Mlxa/brackets-nested\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Mlxa/brackets-nested\")\n",
    "dataset = load_dataset(\"Mlxa/nested\", streaming=True)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import input_output_mapping, sh, fit_linear, fit_module, eval_module, Residual, PrefixMean\n",
    "\n",
    "@typed\n",
    "def get_prompts(n: int) -> list[str]:\n",
    "    return [elem[\"text\"] for elem in islice(dataset, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1, Y_1 = input_output_mapping(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=get_prompts(10),\n",
    "    input_layer=\"transformer.h.0\",\n",
    "    output_layer=\"transformer.h.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:20<00:00, 14.56it/s,  loss=0.07, nonzero=49.09 / 64, nonorth=211.85]\n"
     ]
    }
   ],
   "source": [
    "from utils import fit_sae, SparseAutoEncoder\n",
    "\n",
    "sae = SparseAutoEncoder(256, 64)\n",
    "activations = ein.rearrange(Y_1, \"prompts seq d -> (prompts seq) d\")\n",
    "fit_sae(sae, activations, lr=1e-2, l1=3.0, alpha=1e-4, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:08<00:00, 14.57it/s,  loss=0.02, nonzero=39.66 / 64, nonorth=214.13]\n"
     ]
    }
   ],
   "source": [
    "fit_sae(sae, activations, lr=1e-3, l1=1.0, alpha=1e-4, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18959206342697144\n"
     ]
    }
   ],
   "source": [
    "mid_l = 3\n",
    "mid_r = 6\n",
    "X_mlp_1, Y_mid = input_output_mapping(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=get_prompts(10),\n",
    "    input_layer=f\"transformer.h.{mid_l}\",\n",
    "    output_layer=f\"transformer.h.{mid_r - 1}\",\n",
    ")\n",
    "Y_mid = (Y_mid - X_mlp_1).reshape(-1, 256)\n",
    "X_mlp_1 = X_mlp_1.reshape(-1, 256)\n",
    "mid_line = fit_linear(X_mlp_1, Y_mid, reg=\"l2\", alpha=1e-3)\n",
    "print(\n",
    "    eval_module(\n",
    "        mid_line,\n",
    "        X_mlp_1,\n",
    "        Y_mid,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_ln = nn.LayerNorm(256, elementwise_affine=False)\n",
    "\n",
    "X_mlp_1, Y_mlp_1 = input_output_mapping(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=get_prompts(10),\n",
    "    input_layer=f\"transformer.h.1.ln_2\",\n",
    "    output_layer=f\"transformer.h.1.mlp\",\n",
    ")\n",
    "Y_mlp_1 = Y_mlp_1.reshape(-1, 256)\n",
    "X_mlp_1 = standard_ln(X_mlp_1).reshape(-1, 256)\n",
    "mlp_1 = fit_linear(X_mlp_1, Y_mlp_1, reg=\"l2\", alpha=1e-3)\n",
    "\n",
    "\n",
    "X_mlp_6, Y_mlp_6 = input_output_mapping(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=get_prompts(10),\n",
    "    input_layer=f\"transformer.h.6.ln_2\",\n",
    "    output_layer=f\"transformer.h.6.mlp\",\n",
    ")\n",
    "Y_mlp_6 = Y_mlp_6.reshape(-1, 256)\n",
    "X_mlp_6 = standard_ln(X_mlp_6).reshape(-1, 256)\n",
    "mlp_6 = fit_linear(X_mlp_6, Y_mlp_6, reg=\"l2\", alpha=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001996031031012535\n"
     ]
    }
   ],
   "source": [
    "X_attn, Y_attn = input_output_mapping(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=get_prompts(10),\n",
    "    input_layer=\"transformer.h.0\",\n",
    "    output_layer=\"transformer.h.0.attn\",\n",
    ")\n",
    "\n",
    "X_attn_sums = X_attn.cumsum(dim=-2)\n",
    "X_attn_lens = t.arange(1, X_attn_sums.size(-2) + 1).reshape(1, -1, 1)\n",
    "Y_attn = Y_attn.reshape(-1, 256)\n",
    "X_attn = (X_attn_sums / X_attn_lens).reshape(-1, 256)\n",
    "\n",
    "attn_line = fit_linear(X_attn, Y_attn, reg=\"l2\", alpha=1e-3)\n",
    "print(\n",
    "    eval_module(\n",
    "        attn_line,\n",
    "        X_attn,\n",
    "        Y_attn,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09023763239383698\n"
     ]
    }
   ],
   "source": [
    "X_wo_ln, Y_wo_ln = input_output_mapping(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=get_prompts(10),\n",
    "    input_layer=\"transformer.h.0.ln_2\",\n",
    "    output_layer=\"transformer.h.0.mlp\",\n",
    ")\n",
    "Y_wo_ln = Y_wo_ln.reshape(-1, 256)\n",
    "X_wo_ln = standard_ln(X_wo_ln).reshape(-1, 256)\n",
    "wo_ln = fit_linear(X_wo_ln, Y_wo_ln, reg=\"l2\", alpha=1e-3)\n",
    "print(eval_module(wo_ln, X_wo_ln, Y_wo_ln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean: 3.649, corrupted: 3.735\n",
      "delta: 0.085\n"
     ]
    }
   ],
   "source": [
    "from utils import prompt_from_template, get_loss, PrefixMean, Residual, Wrapper\n",
    "from transformers import GPTNeoForCausalLM\n",
    "\n",
    "new_model = AutoModelForCausalLM.from_pretrained(\"Mlxa/brackets-nested\")\n",
    "new_model.config.use_cache = False\n",
    "new_model.config.output_attentions = False\n",
    "\n",
    "new_model.transformer.h[0] = Wrapper(\n",
    "    nn.Sequential(\n",
    "        Residual(\n",
    "            nn.Sequential(\n",
    "                PrefixMean(),\n",
    "                attn_line,\n",
    "            )\n",
    "        ),\n",
    "        Residual(\n",
    "            nn.Sequential(\n",
    "                nn.LayerNorm(256, elementwise_affine=False),\n",
    "                wo_ln,\n",
    "            )\n",
    "        ),\n",
    "    ),\n",
    "    append=(),\n",
    ")\n",
    "\n",
    "# new_model.transformer.h[6].ln_2 = nn.LayerNorm(256, elementwise_affine=False)\n",
    "# new_model.transformer.h[6].mlp = mlp_6\n",
    "\n",
    "new_model.transformer.h = nn.ModuleList(\n",
    "    new_model.transformer.h[:mid_l]\n",
    "    + [Wrapper(Residual(mid_line), append=())]\n",
    "    + new_model.transformer.h[mid_r:]\n",
    ")\n",
    "\n",
    "prompt = prompt_from_template(\"((((((())))))()\" * 3, random=True)\n",
    "a = get_loss(model, tokenizer, prompt)\n",
    "b = get_loss(new_model, tokenizer, prompt)\n",
    "print(f\"clean: {a:.3f}, corrupted: {b:.3f}\")\n",
    "print(f\"delta: {b - a:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(502, 256)\n",
       "    (wpe): Embedding(2048, 256)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): Wrapper(\n",
       "        (fn): Sequential(\n",
       "          (0): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): PrefixMean()\n",
       "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Residual(\n",
       "            (fn): Sequential(\n",
       "              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=False)\n",
       "              (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-2): 2 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Wrapper(\n",
       "        (fn): Residual(\n",
       "          (fn): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (4-5): 2 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=502, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
