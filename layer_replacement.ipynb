{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from typing import Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from beartype import beartype as typed\n",
    "from beartype.door import die_if_unbearable as assert_type\n",
    "from datasets import load_dataset\n",
    "from einops import einops as ein\n",
    "from jaxtyping import Bool, Float, Int\n",
    "from torch import Tensor as TT\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Mlxa/brackets-nested\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Mlxa/brackets-nested\")\n",
    "dataset = load_dataset(\"Mlxa/nested\", streaming=True)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typed\n",
    "def get_prompts(n: int) -> list[str]:\n",
    "    return [elem[\"text\"] for elem in islice(dataset, n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18959206342697144\n"
     ]
    }
   ],
   "source": [
    "from utils import input_output_mapping, sh, fit_linear, fit_module, eval_module, Residual, PrefixMean\n",
    "\n",
    "replace_l = 3\n",
    "replace_r = 6\n",
    "X_mid, Y_mid = input_output_mapping(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=get_prompts(10),\n",
    "    input_layer=f\"transformer.h.{replace_l}\",\n",
    "    output_layer=f\"transformer.h.{replace_r - 1}\",\n",
    ")\n",
    "Y_mid = (Y_mid - X_mid).reshape(-1, 256)\n",
    "X_mid = X_mid.reshape(-1, 256)\n",
    "mid_line = fit_linear(X_mid, Y_mid, reg=\"l2\", alpha=1e-3)\n",
    "print(\n",
    "    eval_module(\n",
    "        mid_line,\n",
    "        X_mid,\n",
    "        Y_mid,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001996031031012535\n"
     ]
    }
   ],
   "source": [
    "X_attn, Y_attn = input_output_mapping(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=get_prompts(10),\n",
    "    input_layer=\"transformer.h.0\",\n",
    "    output_layer=\"transformer.h.0.attn\",\n",
    ")\n",
    "\n",
    "X_attn_sums = X_attn.cumsum(dim=-2)\n",
    "X_attn_lens = t.arange(1, X_attn_sums.size(-2) + 1).reshape(1, -1, 1)\n",
    "Y_attn = Y_attn.reshape(-1, 256)\n",
    "X_attn = (X_attn_sums / X_attn_lens).reshape(-1, 256)\n",
    "\n",
    "attn_line = fit_linear(X_attn, Y_attn, reg=\"l2\", alpha=1e-3)\n",
    "print(\n",
    "    eval_module(\n",
    "        attn_line,\n",
    "        X_attn,\n",
    "        Y_attn,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09023763239383698\n"
     ]
    }
   ],
   "source": [
    "X_wo_ln, Y_wo_ln = input_output_mapping(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompts=get_prompts(10),\n",
    "    input_layer=\"transformer.h.0.ln_2\",\n",
    "    output_layer=\"transformer.h.0.mlp\",\n",
    ")\n",
    "standard_ln = nn.LayerNorm(256, elementwise_affine=False)\n",
    "Y_wo_ln = Y_wo_ln.reshape(-1, 256)\n",
    "X_wo_ln = standard_ln(X_wo_ln).reshape(-1, 256)\n",
    "wo_ln = fit_linear(X_wo_ln, Y_wo_ln, reg=\"l2\", alpha=1e-3)\n",
    "print(eval_module(wo_ln, X_wo_ln, Y_wo_ln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean: 3.172, corrupted: 3.186\n",
      "delta: 0.014\n"
     ]
    }
   ],
   "source": [
    "from utils import prompt_from_template, get_loss, PrefixMean, Residual, Wrapper\n",
    "from transformers import GPTNeoForCausalLM\n",
    "\n",
    "new_model = AutoModelForCausalLM.from_pretrained(\"Mlxa/brackets-nested\")\n",
    "new_model.config.use_cache = False\n",
    "new_model.config.output_attentions = False\n",
    "\n",
    "new_model.transformer.h[0] = Wrapper(\n",
    "    nn.Sequential(\n",
    "        Residual(\n",
    "            nn.Sequential(\n",
    "                PrefixMean(),\n",
    "                attn_line,\n",
    "            )\n",
    "        ),\n",
    "        Residual(\n",
    "            nn.Sequential(\n",
    "                nn.LayerNorm(256, elementwise_affine=False),\n",
    "                wo_ln,\n",
    "            )\n",
    "        ),\n",
    "    ),\n",
    "    append=(),\n",
    ")\n",
    "\n",
    "new_model.transformer.h = nn.ModuleList(\n",
    "    new_model.transformer.h[:replace_l]\n",
    "    + [Wrapper(Residual(mid_line), append=())]\n",
    "    + new_model.transformer.h[replace_r:]\n",
    ")\n",
    "\n",
    "prompt = prompt_from_template(\"((()))()\" * 3, random=True)\n",
    "a = get_loss(model, tokenizer, prompt)\n",
    "b = get_loss(new_model, tokenizer, prompt)\n",
    "print(f\"clean: {a:.3f}, corrupted: {b:.3f}\")\n",
    "print(f\"delta: {b - a:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoForCausalLM(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(502, 256)\n",
       "    (wpe): Embedding(2048, 256)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Residual(\n",
       "          (fn): Sequential(\n",
       "            (0): PrefixMean()\n",
       "            (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1-2): 2 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): Residual(\n",
       "        (fn): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (4): GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=False)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=256, out_features=502, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
