{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "from beartype import beartype as typed\n",
    "from datasets import load_dataset\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor as TT\n",
    "from transformer_lens import (\n",
    "    ActivationCache,\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    utils,\n",
    ")\n",
    "from einops import rearrange\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"attn.attention.k_proj.weight\": \"attn.W_K\",\n",
    "    \"attn.attention.q_proj.weight\": \"attn.W_Q\",\n",
    "    \"attn.attention.v_proj.weight\": \"attn.W_V\",\n",
    "    \"attn.attention.out_proj.bias\": \"attn.b_O\",\n",
    "    \"attn.attention.out_proj.weight\": \"attn.W_O\",\n",
    "    \"ln_1.bias\": \"ln1.b\",\n",
    "    \"ln_1.weight\": \"ln1.w\",\n",
    "    \"ln_2.bias\": \"ln2.b\",\n",
    "    \"ln_2.weight\": \"ln2.w\",\n",
    "    \"mlp.c_fc.bias\": \"mlp.b_in\",\n",
    "    \"mlp.c_fc.weight\": \"mlp.W_in\",\n",
    "    \"mlp.c_proj.bias\": \"mlp.b_out\",\n",
    "    \"mlp.c_proj.weight\": \"mlp.W_out\",\n",
    "}\n",
    "\n",
    "config = HookedTransformerConfig(\n",
    "    **{\n",
    "        \"act_fn\": \"gelu_new\",\n",
    "        \"attention_dir\": \"causal\",\n",
    "        \"attn_only\": False,\n",
    "        \"attn_types\": [\n",
    "            \"global\",\n",
    "            \"local\",\n",
    "            \"global\",\n",
    "            \"local\",\n",
    "            \"global\",\n",
    "            \"local\",\n",
    "            \"global\",\n",
    "            \"local\",\n",
    "        ],\n",
    "        \"checkpoint_index\": None,\n",
    "        \"checkpoint_label_type\": None,\n",
    "        \"checkpoint_value\": None,\n",
    "        \"d_head\": 16,\n",
    "        \"d_mlp\": 1024,\n",
    "        \"d_model\": 256,\n",
    "        \"d_vocab\": 502,\n",
    "        \"d_vocab_out\": 502,\n",
    "        \"device\": \"cpu\",\n",
    "        \"eps\": 1e-05,\n",
    "        \"final_rms\": False,\n",
    "        \"from_checkpoint\": False,\n",
    "        \"gated_mlp\": False,\n",
    "        \"init_mode\": \"gpt2\",\n",
    "        \"init_weights\": True,\n",
    "        \"initializer_range\": 0.05,\n",
    "        \"model_name\": \"brackets-nested\",\n",
    "        \"n_ctx\": 2048,\n",
    "        \"n_devices\": 1,\n",
    "        \"n_heads\": 16,\n",
    "        \"n_layers\": 8,\n",
    "        \"n_params\": 6291456,\n",
    "        \"normalization_type\": \"LN\",\n",
    "        \"original_architecture\": \"GPTNeoForCausalLM\",\n",
    "        \"parallel_attn_mlp\": False,\n",
    "        \"positional_embedding_type\": \"standard\",\n",
    "        \"rotary_dim\": None,\n",
    "        \"scale_attn_by_inverse_layer_idx\": False,\n",
    "        \"seed\": None,\n",
    "        \"tokenizer_name\": \"Mlxa/brackets-nested\",\n",
    "        \"use_attn_result\": False,\n",
    "        \"use_attn_scale\": False,\n",
    "        \"use_hook_tokens\": False,\n",
    "        \"use_local_attn\": True,\n",
    "        \"use_split_qkv_input\": False,\n",
    "        \"window_size\": 256,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using eos_token, but it is not set yet.\n",
      "Using bos_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight -> embed.W_E : torch.Size([502, 256]) torch.Size([502, 256])\n",
      "transformer.wpe.weight -> pos_embed.W_pos : torch.Size([2048, 256]) torch.Size([2048, 256])\n",
      "transformer.h.0.ln_1.weight -> blocks.0.ln1.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.0.ln_1.bias -> blocks.0.ln1.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.0.attn.attention.k_proj.weight -> blocks.0.attn.W_K : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.0.attn.attention.v_proj.weight -> blocks.0.attn.W_V : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.0.attn.attention.q_proj.weight -> blocks.0.attn.W_Q : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.0.attn.attention.out_proj.weight -> blocks.0.attn.W_O : torch.Size([16, 16, 256]) torch.Size([16, 16, 256])\n",
      "transformer.h.0.attn.attention.out_proj.bias -> blocks.0.attn.b_O : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.0.ln_2.weight -> blocks.0.ln2.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.0.ln_2.bias -> blocks.0.ln2.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.0.mlp.c_fc.weight -> blocks.0.mlp.W_in : torch.Size([256, 1024]) torch.Size([256, 1024])\n",
      "transformer.h.0.mlp.c_fc.bias -> blocks.0.mlp.b_in : torch.Size([1024]) torch.Size([1024])\n",
      "transformer.h.0.mlp.c_proj.weight -> blocks.0.mlp.W_out : torch.Size([1024, 256]) torch.Size([1024, 256])\n",
      "transformer.h.0.mlp.c_proj.bias -> blocks.0.mlp.b_out : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.1.ln_1.weight -> blocks.1.ln1.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.1.ln_1.bias -> blocks.1.ln1.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.1.attn.attention.k_proj.weight -> blocks.1.attn.W_K : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.1.attn.attention.v_proj.weight -> blocks.1.attn.W_V : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.1.attn.attention.q_proj.weight -> blocks.1.attn.W_Q : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.1.attn.attention.out_proj.weight -> blocks.1.attn.W_O : torch.Size([16, 16, 256]) torch.Size([16, 16, 256])\n",
      "transformer.h.1.attn.attention.out_proj.bias -> blocks.1.attn.b_O : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.1.ln_2.weight -> blocks.1.ln2.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.1.ln_2.bias -> blocks.1.ln2.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.1.mlp.c_fc.weight -> blocks.1.mlp.W_in : torch.Size([256, 1024]) torch.Size([256, 1024])\n",
      "transformer.h.1.mlp.c_fc.bias -> blocks.1.mlp.b_in : torch.Size([1024]) torch.Size([1024])\n",
      "transformer.h.1.mlp.c_proj.weight -> blocks.1.mlp.W_out : torch.Size([1024, 256]) torch.Size([1024, 256])\n",
      "transformer.h.1.mlp.c_proj.bias -> blocks.1.mlp.b_out : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.2.ln_1.weight -> blocks.2.ln1.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.2.ln_1.bias -> blocks.2.ln1.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.2.attn.attention.k_proj.weight -> blocks.2.attn.W_K : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.2.attn.attention.v_proj.weight -> blocks.2.attn.W_V : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.2.attn.attention.q_proj.weight -> blocks.2.attn.W_Q : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.2.attn.attention.out_proj.weight -> blocks.2.attn.W_O : torch.Size([16, 16, 256]) torch.Size([16, 16, 256])\n",
      "transformer.h.2.attn.attention.out_proj.bias -> blocks.2.attn.b_O : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.2.ln_2.weight -> blocks.2.ln2.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.2.ln_2.bias -> blocks.2.ln2.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.2.mlp.c_fc.weight -> blocks.2.mlp.W_in : torch.Size([256, 1024]) torch.Size([256, 1024])\n",
      "transformer.h.2.mlp.c_fc.bias -> blocks.2.mlp.b_in : torch.Size([1024]) torch.Size([1024])\n",
      "transformer.h.2.mlp.c_proj.weight -> blocks.2.mlp.W_out : torch.Size([1024, 256]) torch.Size([1024, 256])\n",
      "transformer.h.2.mlp.c_proj.bias -> blocks.2.mlp.b_out : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.3.ln_1.weight -> blocks.3.ln1.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.3.ln_1.bias -> blocks.3.ln1.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.3.attn.attention.k_proj.weight -> blocks.3.attn.W_K : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.3.attn.attention.v_proj.weight -> blocks.3.attn.W_V : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.3.attn.attention.q_proj.weight -> blocks.3.attn.W_Q : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.3.attn.attention.out_proj.weight -> blocks.3.attn.W_O : torch.Size([16, 16, 256]) torch.Size([16, 16, 256])\n",
      "transformer.h.3.attn.attention.out_proj.bias -> blocks.3.attn.b_O : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.3.ln_2.weight -> blocks.3.ln2.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.3.ln_2.bias -> blocks.3.ln2.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.3.mlp.c_fc.weight -> blocks.3.mlp.W_in : torch.Size([256, 1024]) torch.Size([256, 1024])\n",
      "transformer.h.3.mlp.c_fc.bias -> blocks.3.mlp.b_in : torch.Size([1024]) torch.Size([1024])\n",
      "transformer.h.3.mlp.c_proj.weight -> blocks.3.mlp.W_out : torch.Size([1024, 256]) torch.Size([1024, 256])\n",
      "transformer.h.3.mlp.c_proj.bias -> blocks.3.mlp.b_out : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.4.ln_1.weight -> blocks.4.ln1.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.4.ln_1.bias -> blocks.4.ln1.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.4.attn.attention.k_proj.weight -> blocks.4.attn.W_K : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.4.attn.attention.v_proj.weight -> blocks.4.attn.W_V : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.4.attn.attention.q_proj.weight -> blocks.4.attn.W_Q : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.4.attn.attention.out_proj.weight -> blocks.4.attn.W_O : torch.Size([16, 16, 256]) torch.Size([16, 16, 256])\n",
      "transformer.h.4.attn.attention.out_proj.bias -> blocks.4.attn.b_O : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.4.ln_2.weight -> blocks.4.ln2.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.4.ln_2.bias -> blocks.4.ln2.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.4.mlp.c_fc.weight -> blocks.4.mlp.W_in : torch.Size([256, 1024]) torch.Size([256, 1024])\n",
      "transformer.h.4.mlp.c_fc.bias -> blocks.4.mlp.b_in : torch.Size([1024]) torch.Size([1024])\n",
      "transformer.h.4.mlp.c_proj.weight -> blocks.4.mlp.W_out : torch.Size([1024, 256]) torch.Size([1024, 256])\n",
      "transformer.h.4.mlp.c_proj.bias -> blocks.4.mlp.b_out : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.5.ln_1.weight -> blocks.5.ln1.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.5.ln_1.bias -> blocks.5.ln1.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.5.attn.attention.k_proj.weight -> blocks.5.attn.W_K : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.5.attn.attention.v_proj.weight -> blocks.5.attn.W_V : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.5.attn.attention.q_proj.weight -> blocks.5.attn.W_Q : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.5.attn.attention.out_proj.weight -> blocks.5.attn.W_O : torch.Size([16, 16, 256]) torch.Size([16, 16, 256])\n",
      "transformer.h.5.attn.attention.out_proj.bias -> blocks.5.attn.b_O : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.5.ln_2.weight -> blocks.5.ln2.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.5.ln_2.bias -> blocks.5.ln2.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.5.mlp.c_fc.weight -> blocks.5.mlp.W_in : torch.Size([256, 1024]) torch.Size([256, 1024])\n",
      "transformer.h.5.mlp.c_fc.bias -> blocks.5.mlp.b_in : torch.Size([1024]) torch.Size([1024])\n",
      "transformer.h.5.mlp.c_proj.weight -> blocks.5.mlp.W_out : torch.Size([1024, 256]) torch.Size([1024, 256])\n",
      "transformer.h.5.mlp.c_proj.bias -> blocks.5.mlp.b_out : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.6.ln_1.weight -> blocks.6.ln1.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.6.ln_1.bias -> blocks.6.ln1.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.6.attn.attention.k_proj.weight -> blocks.6.attn.W_K : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.6.attn.attention.v_proj.weight -> blocks.6.attn.W_V : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.6.attn.attention.q_proj.weight -> blocks.6.attn.W_Q : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.6.attn.attention.out_proj.weight -> blocks.6.attn.W_O : torch.Size([16, 16, 256]) torch.Size([16, 16, 256])\n",
      "transformer.h.6.attn.attention.out_proj.bias -> blocks.6.attn.b_O : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.6.ln_2.weight -> blocks.6.ln2.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.6.ln_2.bias -> blocks.6.ln2.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.6.mlp.c_fc.weight -> blocks.6.mlp.W_in : torch.Size([256, 1024]) torch.Size([256, 1024])\n",
      "transformer.h.6.mlp.c_fc.bias -> blocks.6.mlp.b_in : torch.Size([1024]) torch.Size([1024])\n",
      "transformer.h.6.mlp.c_proj.weight -> blocks.6.mlp.W_out : torch.Size([1024, 256]) torch.Size([1024, 256])\n",
      "transformer.h.6.mlp.c_proj.bias -> blocks.6.mlp.b_out : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.7.ln_1.weight -> blocks.7.ln1.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.7.ln_1.bias -> blocks.7.ln1.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.7.attn.attention.k_proj.weight -> blocks.7.attn.W_K : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.7.attn.attention.v_proj.weight -> blocks.7.attn.W_V : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.7.attn.attention.q_proj.weight -> blocks.7.attn.W_Q : torch.Size([16, 256, 16]) torch.Size([16, 256, 16])\n",
      "transformer.h.7.attn.attention.out_proj.weight -> blocks.7.attn.W_O : torch.Size([16, 16, 256]) torch.Size([16, 16, 256])\n",
      "transformer.h.7.attn.attention.out_proj.bias -> blocks.7.attn.b_O : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.7.ln_2.weight -> blocks.7.ln2.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.7.ln_2.bias -> blocks.7.ln2.b : torch.Size([256]) torch.Size([256])\n",
      "transformer.h.7.mlp.c_fc.weight -> blocks.7.mlp.W_in : torch.Size([256, 1024]) torch.Size([256, 1024])\n",
      "transformer.h.7.mlp.c_fc.bias -> blocks.7.mlp.b_in : torch.Size([1024]) torch.Size([1024])\n",
      "transformer.h.7.mlp.c_proj.weight -> blocks.7.mlp.W_out : torch.Size([1024, 256]) torch.Size([1024, 256])\n",
      "transformer.h.7.mlp.c_proj.bias -> blocks.7.mlp.b_out : torch.Size([256]) torch.Size([256])\n",
      "transformer.ln_f.weight -> ln_final.w : torch.Size([256]) torch.Size([256])\n",
      "transformer.ln_f.bias -> ln_final.b : torch.Size([256]) torch.Size([256])\n",
      "lm_head.weight -> unembed.W_U : torch.Size([256, 502]) torch.Size([256, 502])\n",
      "zero blocks.0.attn.b_Q torch.Size([16, 16])\n",
      "zero blocks.0.attn.b_K torch.Size([16, 16])\n",
      "zero blocks.0.attn.b_V torch.Size([16, 16])\n",
      "constant blocks.0.attn.mask\n",
      "constant blocks.0.attn.IGNORE\n",
      "zero blocks.1.attn.b_Q torch.Size([16, 16])\n",
      "zero blocks.1.attn.b_K torch.Size([16, 16])\n",
      "zero blocks.1.attn.b_V torch.Size([16, 16])\n",
      "constant blocks.1.attn.mask\n",
      "constant blocks.1.attn.IGNORE\n",
      "zero blocks.2.attn.b_Q torch.Size([16, 16])\n",
      "zero blocks.2.attn.b_K torch.Size([16, 16])\n",
      "zero blocks.2.attn.b_V torch.Size([16, 16])\n",
      "constant blocks.2.attn.mask\n",
      "constant blocks.2.attn.IGNORE\n",
      "zero blocks.3.attn.b_Q torch.Size([16, 16])\n",
      "zero blocks.3.attn.b_K torch.Size([16, 16])\n",
      "zero blocks.3.attn.b_V torch.Size([16, 16])\n",
      "constant blocks.3.attn.mask\n",
      "constant blocks.3.attn.IGNORE\n",
      "zero blocks.4.attn.b_Q torch.Size([16, 16])\n",
      "zero blocks.4.attn.b_K torch.Size([16, 16])\n",
      "zero blocks.4.attn.b_V torch.Size([16, 16])\n",
      "constant blocks.4.attn.mask\n",
      "constant blocks.4.attn.IGNORE\n",
      "zero blocks.5.attn.b_Q torch.Size([16, 16])\n",
      "zero blocks.5.attn.b_K torch.Size([16, 16])\n",
      "zero blocks.5.attn.b_V torch.Size([16, 16])\n",
      "constant blocks.5.attn.mask\n",
      "constant blocks.5.attn.IGNORE\n",
      "zero blocks.6.attn.b_Q torch.Size([16, 16])\n",
      "zero blocks.6.attn.b_K torch.Size([16, 16])\n",
      "zero blocks.6.attn.b_V torch.Size([16, 16])\n",
      "constant blocks.6.attn.mask\n",
      "constant blocks.6.attn.IGNORE\n",
      "zero blocks.7.attn.b_Q torch.Size([16, 16])\n",
      "zero blocks.7.attn.b_K torch.Size([16, 16])\n",
      "zero blocks.7.attn.b_V torch.Size([16, 16])\n",
      "constant blocks.7.attn.mask\n",
      "constant blocks.7.attn.IGNORE\n",
      "zero unembed.b_U torch.Size([502])\n",
      "150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HookedTransformer(config)\n",
    "param_holder = AutoModelForCausalLM.from_pretrained(\"Mlxa/brackets-nested\")\n",
    "old_dict = param_holder.state_dict()\n",
    "new_dict = model.state_dict()\n",
    "ka = list(old_dict.keys())\n",
    "kb = list(new_dict.keys())\n",
    "\n",
    "loaded = set()\n",
    "for x in ka:\n",
    "    if \"transformer.h.\" in x:\n",
    "        y = x.replace(\"transformer.h.\", \"blocks.\")\n",
    "        suffix = y[len(\"blocks.#.\"):]\n",
    "        y = y.replace(suffix, mapping[suffix])\n",
    "    elif \"lm_head\" in x:\n",
    "        y = \"unembed.W_U\"\n",
    "    elif \"wpe\" in x:\n",
    "        y = \"pos_embed.W_pos\"\n",
    "    elif \"wte\" in x:\n",
    "        y = \"embed.W_E\"\n",
    "    elif \"ln_f.bias\" in x:\n",
    "        y = \"ln_final.b\"\n",
    "    elif \"ln_f.weight\" in x:\n",
    "        y = \"ln_final.w\"\n",
    "    else:\n",
    "        print(\"UNKNOWN\", x)\n",
    "    data = old_dict[x]\n",
    "    if data.shape != new_dict[y].shape:\n",
    "        data = data.T\n",
    "    if data.shape != new_dict[y].shape:\n",
    "        if new_dict[y].shape[1] > new_dict[y].shape[2]:\n",
    "            data = rearrange(data, \"d_in (h d_out) -> h d_in d_out\", h=new_dict[y].shape[0])\n",
    "        else:\n",
    "            data = rearrange(data, \"(h d_in) d_out -> h d_in d_out\", h=new_dict[y].shape[0])\n",
    "    print(x, \"->\", y, \":\", data.shape, new_dict[y].shape)\n",
    "    assert new_dict[y].shape == data.shape\n",
    "    new_dict[y] = data\n",
    "    assert y in kb\n",
    "    loaded.add(y)\n",
    "\n",
    "for x in kb:\n",
    "    if x in loaded:\n",
    "        continue\n",
    "    elif \"b_\" in x:\n",
    "        print(\"zero\", x, new_dict[x].shape)\n",
    "        new_dict[x] = t.zeros_like(new_dict[x])\n",
    "        loaded.add(x)\n",
    "    else:\n",
    "        print(\"constant\", x)\n",
    "        loaded.add(x)\n",
    "\n",
    "print(len(loaded))\n",
    "model.load_state_dict(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<1 <2 <3 <216 <78 <85 <100 <161 161> <132 <29 29> <42 42> <130 <42 <17 <91 <68 <125 125> <48 16>'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.generate(\"<1 <2 <3\", prepend_bos=False)\n",
    "from utils import generate_sample\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "inputs = tokenizer(\"<1 <2 <3\", return_tensors=\"pt\")\n",
    "# del inputs[\"token_type_ids\"]\n",
    "result = param_holder.generate(**inputs, do_sample=True, max_new_tokens=20)\n",
    "tokenizer.decode(result.squeeze())\n",
    "# result = model.generate(inputs[\"input_ids\"], do_sample=True)\n",
    "# tokenizer.decode(result.squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
