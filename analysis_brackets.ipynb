{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from beartype import beartype as typed\n",
    "from datasets import load_dataset\n",
    "from jaxtyping import Float, Int\n",
    "from typing import Callable\n",
    "from torch import Tensor as TT\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"Mlxa/brackets-nested\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Mlxa/brackets-nested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "@typed\n",
    "def gauss_helper(a: TT | tuple) -> TT | tuple:\n",
    "    if isinstance(a, tuple):\n",
    "        return tuple(gauss_helper(x) for x in a)\n",
    "    else:\n",
    "        assert a.numel() > 2\n",
    "        mean = a.mean()\n",
    "        std = a.std()\n",
    "        return 0 * (t.randn_like(a) * std + mean)\n",
    "\n",
    "@typed\n",
    "def gauss_hook(module: nn.Module, _input: TT | tuple, output: TT | tuple) -> TT | tuple:\n",
    "    return gauss_helper(output)\n",
    "\n",
    "class Hooks:\n",
    "    @typed\n",
    "    def __init__(self, module: nn.Module, filter: Callable[[str], bool], hook: Callable[[nn.Module, TT, TT], TT]) -> None:\n",
    "        self.handles = []\n",
    "        self.module = module\n",
    "        for name, submodule in module.named_modules():\n",
    "            if filter(name):\n",
    "                # print(name, \"hooked\")\n",
    "                self.handles.append(submodule.register_forward_hook(hook))\n",
    "    \n",
    "    @typed\n",
    "    def __enter__(self) -> nn.Module:\n",
    "        return self.module\n",
    "    \n",
    "    @typed\n",
    "    def __exit__(self, *_) -> None:\n",
    "        for handle in self.handles:\n",
    "            handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_sample, get_loss, tokenize, get_logprobs\n",
    "\n",
    "\n",
    "@typed\n",
    "def nested_prompt(n: int) -> str:\n",
    "    openning = \" \".join(f\"<{i}\" for i in range(1, n + 1))\n",
    "    closing = \" \".join(f\"{i}>\" for i in range(n, 0, -1))\n",
    "    return openning + \" \" + closing\n",
    "\n",
    "@typed\n",
    "def loss_metric(model: nn.Module, tokenizer, n: int) -> float:\n",
    "    prompt = nested_prompt(n)\n",
    "    return get_loss(model, tokenizer, prompt) * n\n",
    "\n",
    "@typed\n",
    "def accuracy_metric(model: nn.Module, tokenizer, n: int) -> float:\n",
    "    prompt = nested_prompt(n)\n",
    "    ids = tokenize(tokenizer, prompt)[\"input_ids\"][0]\n",
    "    lp = get_logprobs(model, tokenizer, prompt)\n",
    "    mx = lp.argmax(dim=-1)\n",
    "    correct = mx == ids\n",
    "    assert correct.shape == (2 * n,)\n",
    "    return correct[n:].float().mean().item()\n",
    "\n",
    "@typed\n",
    "def max_correct(model: nn.Module, tokenizer) -> int:\n",
    "    l, r = 0, 64\n",
    "    while r - l > 1:\n",
    "        m = (l + r) // 2\n",
    "        if accuracy_metric(model, tokenizer, m) > 1 - 1e-9:\n",
    "            l = m\n",
    "        else:\n",
    "            r = m\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head [12.72193717956543, 23.6521053314209, 41.880531311035156, 42.88937759399414, 35.908416748046875]\n",
      "transformer.h.0.attn.attention.k_proj [1.5446062088012695, 1.3048477172851562, 1.8312530517578125, -0.48790740966796875, 0.5988006591796875]\n",
      "transformer.h.0.attn.attention.out_proj [18.735538482666016, 32.247182846069336, 69.03973388671875, 88.65131378173828, 107.03175354003906]\n",
      "transformer.h.0.attn.attention.q_proj [1.5446062088012695, 1.3048477172851562, 1.8312530517578125, -0.48790740966796875, 0.5988006591796875]\n",
      "transformer.h.0.attn.attention.v_proj [20.24831771850586, 51.756425857543945, 94.49577331542969, 120.15218353271484, 142.19187927246094]\n",
      "transformer.h.0.ln_1 [20.24831771850586, 51.756425857543945, 94.49577331542969, 120.15218353271484, 142.19187927246094]\n",
      "transformer.h.0.ln_2 [0.1554584503173828, 4.603397369384766, 43.11750793457031, 50.2056999206543, 53.174407958984375]\n",
      "transformer.h.0.mlp.c_fc [0.1218404769897461, 5.505771636962891, 46.03436279296875, 53.6118049621582, 61.75146484375]\n",
      "transformer.h.0.mlp.c_proj [0.12537288665771484, 5.443347930908203, 48.10182189941406, 55.965511322021484, 65.44085693359375]\n",
      "transformer.h.1.attn.attention.k_proj [2.811945915222168, 5.2122802734375, 30.742446899414062, 52.71909713745117, 53.769805908203125]\n",
      "transformer.h.1.attn.attention.out_proj [4.777256011962891, 9.18452262878418, 28.60466766357422, 22.050819396972656, 12.69189453125]\n",
      "transformer.h.1.attn.attention.q_proj [2.811945915222168, 5.2122802734375, 30.742446899414062, 52.71909713745117, 53.769805908203125]\n",
      "transformer.h.1.attn.attention.v_proj [4.587116241455078, 8.993803024291992, 29.436325073242188, 23.875465393066406, 12.006515502929688]\n",
      "transformer.h.1.ln_1 [4.587116241455078, 8.993803024291992, 29.436325073242188, 23.875465393066406, 12.006515502929688]\n",
      "transformer.h.1.ln_2 [0.2552928924560547, 1.3481311798095703, 3.5569915771484375, -0.8277053833007812, -0.661468505859375]\n",
      "transformer.h.1.mlp.c_fc [0.5369672775268555, 2.551952362060547, 5.387607574462891, 0.7512474060058594, 2.8629150390625]\n",
      "transformer.h.1.mlp.c_proj [0.5689001083374023, 2.744840621948242, 5.657825469970703, 0.9282417297363281, 2.8855438232421875]\n",
      "transformer.h.2.attn.attention.k_proj [0.731541633605957, 2.4468555450439453, 7.331291198730469, 9.202640533447266, 10.60235595703125]\n",
      "transformer.h.2.attn.attention.out_proj [0.18828392028808594, 0.24027442932128906, 9.424705505371094, 13.731616973876953, 11.427276611328125]\n",
      "transformer.h.2.attn.attention.q_proj [0.731541633605957, 2.4468555450439453, 7.331291198730469, 9.202640533447266, 10.60235595703125]\n",
      "transformer.h.2.attn.attention.v_proj [0.21497249603271484, 0.24117469787597656, 9.118049621582031, 13.177791595458984, 11.112380981445312]\n",
      "transformer.h.2.ln_1 [0.21497249603271484, 0.24117469787597656, 9.118049621582031, 13.177791595458984, 11.112380981445312]\n",
      "transformer.h.2.ln_2 [0.5719757080078125, 6.102014541625977, 34.29801940917969, 44.17975616455078, 46.29060363769531]\n",
      "transformer.h.2.mlp.c_fc [0.660670280456543, 6.194028854370117, 34.89098358154297, 45.66075897216797, 45.92710876464844]\n",
      "transformer.h.2.mlp.c_proj [0.6820125579833984, 6.274110794067383, 34.967750549316406, 45.74802017211914, 45.96464538574219]\n",
      "transformer.h.3.attn.attention.k_proj [0.832066535949707, 0.7438526153564453, 2.324726104736328, 8.542625427246094, 8.41094970703125]\n",
      "transformer.h.3.attn.attention.out_proj [1.2995128631591797, 1.4482688903808594, 6.034122467041016, 9.371063232421875, 13.946395874023438]\n",
      "transformer.h.3.attn.attention.q_proj [0.832066535949707, 0.7438526153564453, 2.324726104736328, 8.542625427246094, 8.41094970703125]\n",
      "transformer.h.3.attn.attention.v_proj [1.3432598114013672, 1.4696540832519531, 5.700325012207031, 8.926849365234375, 13.700531005859375]\n",
      "transformer.h.3.ln_1 [1.3432598114013672, 1.4696540832519531, 5.700325012207031, 8.926849365234375, 13.700531005859375]\n",
      "transformer.h.3.ln_2 [-0.011702537536621094, -0.099609375, -0.8509559631347656, 5.199028015136719, 4.696014404296875]\n",
      "transformer.h.3.mlp.c_fc [-0.0026617050170898438, -0.08510017395019531, -0.8288497924804688, 6.098487854003906, 5.7534942626953125]\n",
      "transformer.h.3.mlp.c_proj [0.007885932922363281, -0.0659637451171875, -0.639068603515625, 6.491970062255859, 6.1858367919921875]\n",
      "transformer.h.4.attn.attention.k_proj [0.09715747833251953, -0.14890670776367188, -0.12755203247070312, 0.16817092895507812, -0.0473785400390625]\n",
      "transformer.h.4.attn.attention.out_proj [-0.008575439453125, 0.135345458984375, 2.288684844970703, 7.322410583496094, 10.999160766601562]\n",
      "transformer.h.4.attn.attention.q_proj [0.09715747833251953, -0.14890670776367188, -0.12755203247070312, 0.16817092895507812, -0.0473785400390625]\n",
      "transformer.h.4.attn.attention.v_proj [-0.035910606384277344, 0.09185600280761719, 2.273082733154297, 7.2527618408203125, 10.975112915039062]\n",
      "transformer.h.4.ln_1 [-0.035910606384277344, 0.09185600280761719, 2.273082733154297, 7.2527618408203125, 10.975112915039062]\n",
      "transformer.h.4.ln_2 [0.0941925048828125, 0.6480693817138672, 6.4067230224609375, 11.627758026123047, 13.485702514648438]\n",
      "transformer.h.4.mlp.c_fc [0.1886587142944336, 0.782379150390625, 6.4439849853515625, 11.696949005126953, 14.420654296875]\n",
      "transformer.h.4.mlp.c_proj [0.2434978485107422, 0.8158950805664062, 6.474365234375, 11.706710815429688, 14.633071899414062]\n",
      "transformer.h.5.attn.attention.k_proj [-0.011465072631835938, -0.042629241943359375, -0.10530471801757812, 1.0503959655761719, 1.13232421875]\n",
      "transformer.h.5.attn.attention.out_proj [0.1922740936279297, 0.4715690612792969, 0.30101776123046875, 1.8874969482421875, 2.43487548828125]\n",
      "transformer.h.5.attn.attention.q_proj [-0.011465072631835938, -0.042629241943359375, -0.10530471801757812, 1.0503959655761719, 1.13232421875]\n",
      "transformer.h.5.attn.attention.v_proj [0.12893009185791016, 0.38256263732910156, 0.371673583984375, 1.66058349609375, 2.331085205078125]\n",
      "transformer.h.5.ln_1 [0.12893009185791016, 0.38256263732910156, 0.371673583984375, 1.66058349609375, 2.331085205078125]\n",
      "transformer.h.5.ln_2 [0.9047002792358398, 5.927122116088867, 9.549522399902344, 5.225589752197266, 8.358642578125]\n",
      "transformer.h.5.mlp.c_fc [1.1071643829345703, 5.357461929321289, 10.165451049804688, 7.347324371337891, 10.11712646484375]\n",
      "transformer.h.5.mlp.c_proj [1.136469841003418, 5.227745056152344, 10.152595520019531, 7.314571380615234, 10.18853759765625]\n",
      "transformer.h.6.attn.attention.k_proj [3.9497623443603516, 5.774509429931641, 11.8980712890625, 16.209823608398438, 14.15911865234375]\n",
      "transformer.h.6.attn.attention.out_proj [1.1722841262817383, 7.784631729125977, 34.90855407714844, 49.78611373901367, 53.085601806640625]\n",
      "transformer.h.6.attn.attention.q_proj [3.9497623443603516, 5.774509429931641, 11.8980712890625, 16.209823608398438, 14.15911865234375]\n",
      "transformer.h.6.attn.attention.v_proj [1.1715192794799805, 7.749021530151367, 34.239830017089844, 49.29218673706055, 52.3543701171875]\n",
      "transformer.h.6.ln_1 [1.1715192794799805, 7.749021530151367, 34.239830017089844, 49.29218673706055, 52.3543701171875]\n",
      "transformer.h.6.ln_2 [0.47385501861572266, 0.4476318359375, 19.856346130371094, 19.831912994384766, 15.495941162109375]\n",
      "transformer.h.6.mlp.c_fc [0.6422824859619141, 0.3203277587890625, 23.153671264648438, 29.72072982788086, 19.868118286132812]\n",
      "transformer.h.6.mlp.c_proj [0.6707868576049805, 0.38051414489746094, 23.60651397705078, 30.962265014648438, 20.892929077148438]\n",
      "transformer.h.7.attn.attention.k_proj [7.909870147705078, 18.932165145874023, 44.213287353515625, 50.60551071166992, 49.195068359375]\n",
      "transformer.h.7.attn.attention.out_proj [4.502496719360352, 17.980566024780273, 51.641212463378906, 71.23223876953125, 79.50442504882812]\n",
      "transformer.h.7.attn.attention.q_proj [7.909870147705078, 18.932165145874023, 44.213287353515625, 50.60551071166992, 49.195068359375]\n",
      "transformer.h.7.attn.attention.v_proj [4.432464599609375, 17.901559829711914, 51.53430938720703, 71.08364868164062, 79.35751342773438]\n",
      "transformer.h.7.ln_1 [4.432464599609375, 17.901559829711914, 51.53430938720703, 71.08364868164062, 79.35751342773438]\n",
      "transformer.h.7.ln_2 [11.839427947998047, 24.1733341217041, 46.950843811035156, 73.46406555175781, 95.52558898925781]\n",
      "transformer.h.7.mlp.c_fc [11.87795639038086, 24.19291877746582, 46.726646423339844, 73.33355712890625, 95.57383728027344]\n",
      "transformer.h.7.mlp.c_proj [12.247695922851562, 25.028982162475586, 48.26422119140625, 75.42341995239258, 98.10173034667969]\n",
      "transformer.ln_f [12.72193717956543, 23.6521053314209, 41.880531311035156, 42.88937759399414, 35.908416748046875]\n",
      "transformer.wpe [3.2893753051757812, 13.801149368286133, 42.34832763671875, 43.12679672241211, 40.375396728515625]\n",
      "transformer.wte [29.91967010498047, 53.04394721984863, 102.14707946777344, 131.63587188720703, 155.73228454589844]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "all_module_names = [\n",
    "    \".\".join(name.split(\".\")[:-1]) for name in model.state_dict().keys()\n",
    "]\n",
    "all_module_names = sorted(list(set(all_module_names)))\n",
    "ns = [4, 8, 16, 24, 32]\n",
    "results = defaultdict(list)\n",
    "baselines = {n: loss_metric(model, tokenizer, n) for n in ns}\n",
    "\n",
    "for name in all_module_names:\n",
    "    for n in ns:\n",
    "        with Hooks(model, lambda x: x == name, gauss_hook) as hooked:\n",
    "            results[name].append(loss_metric(hooked, tokenizer, n) - baselines[n])\n",
    "    print(name, results[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lm_head 0\n",
      "transformer.h.0.attn.attention.k_proj 15\n",
      "transformer.h.0.attn.attention.out_proj 0\n",
      "transformer.h.0.attn.attention.q_proj 15\n",
      "transformer.h.0.attn.attention.v_proj 0\n",
      "transformer.h.0.ln_1 0\n",
      "transformer.h.0.ln_2 8\n",
      "transformer.h.0.mlp.c_fc 8\n",
      "transformer.h.0.mlp.c_proj 8\n",
      "transformer.h.1.attn.attention.k_proj 3\n",
      "transformer.h.1.attn.attention.out_proj 2\n",
      "transformer.h.1.attn.attention.q_proj 3\n",
      "transformer.h.1.attn.attention.v_proj 2\n",
      "transformer.h.1.ln_1 2\n",
      "transformer.h.1.ln_2 9\n",
      "transformer.h.1.mlp.c_fc 10\n",
      "transformer.h.1.mlp.c_proj 10\n",
      "transformer.h.2.attn.attention.k_proj 9\n",
      "transformer.h.2.attn.attention.out_proj 12\n",
      "transformer.h.2.attn.attention.q_proj 9\n",
      "transformer.h.2.attn.attention.v_proj 12\n",
      "transformer.h.2.ln_1 12\n",
      "transformer.h.2.ln_2 7\n",
      "transformer.h.2.mlp.c_fc 6\n",
      "transformer.h.2.mlp.c_proj 6\n",
      "transformer.h.3.attn.attention.k_proj 16\n",
      "transformer.h.3.attn.attention.out_proj 13\n",
      "transformer.h.3.attn.attention.q_proj 16\n",
      "transformer.h.3.attn.attention.v_proj 13\n",
      "transformer.h.3.ln_1 13\n",
      "transformer.h.3.ln_2 16\n",
      "transformer.h.3.mlp.c_fc 16\n",
      "transformer.h.3.mlp.c_proj 16\n",
      "transformer.h.4.attn.attention.k_proj 16\n",
      "transformer.h.4.attn.attention.out_proj 16\n",
      "transformer.h.4.attn.attention.q_proj 16\n",
      "transformer.h.4.attn.attention.v_proj 16\n",
      "transformer.h.4.ln_1 16\n",
      "transformer.h.4.ln_2 15\n",
      "transformer.h.4.mlp.c_fc 15\n",
      "transformer.h.4.mlp.c_proj 14\n",
      "transformer.h.5.attn.attention.k_proj 16\n",
      "transformer.h.5.attn.attention.out_proj 16\n",
      "transformer.h.5.attn.attention.q_proj 16\n",
      "transformer.h.5.attn.attention.v_proj 16\n",
      "transformer.h.5.ln_1 16\n",
      "transformer.h.5.ln_2 6\n",
      "transformer.h.5.mlp.c_fc 7\n",
      "transformer.h.5.mlp.c_proj 7\n",
      "transformer.h.6.attn.attention.k_proj 3\n",
      "transformer.h.6.attn.attention.out_proj 4\n",
      "transformer.h.6.attn.attention.q_proj 3\n",
      "transformer.h.6.attn.attention.v_proj 4\n",
      "transformer.h.6.ln_1 4\n",
      "transformer.h.6.ln_2 9\n",
      "transformer.h.6.mlp.c_fc 9\n",
      "transformer.h.6.mlp.c_proj 9\n",
      "transformer.h.7.attn.attention.k_proj 1\n",
      "transformer.h.7.attn.attention.out_proj 3\n",
      "transformer.h.7.attn.attention.q_proj 1\n",
      "transformer.h.7.attn.attention.v_proj 3\n",
      "transformer.h.7.ln_1 3\n",
      "transformer.h.7.ln_2 0\n",
      "transformer.h.7.mlp.c_fc 0\n",
      "transformer.h.7.mlp.c_proj 0\n",
      "transformer.ln_f 0\n",
      "transformer.wpe 3\n",
      "transformer.wte 0\n"
     ]
    }
   ],
   "source": [
    "for name in all_module_names:\n",
    "    for n in ns:\n",
    "        with Hooks(model, lambda x: x == name, gauss_hook) as hooked:\n",
    "            results[name] = max_correct(hooked, tokenizer)\n",
    "    print(name, results[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.0.attn.attention.out_proj   9.157332083074044 3.2133197871650134\n",
      "transformer.h.0.attn.attention.v_proj     14.183864535354967 4.261014964522386\n",
      "transformer.h.0.ln_1                      14.183864535354967 4.261014964522386\n",
      "transformer.h.0.ln_2                      -4.729656359044528 2.082199445584925\n",
      "transformer.h.0.mlp.c_fc                  -6.219307573830216 2.358592649785484\n",
      "transformer.h.0.mlp.c_proj                -6.9098697987998605 2.495550713888029\n",
      "transformer.h.1.attn.attention.k_proj     -5.85573838396771 2.077788905399602\n",
      "transformer.h.1.attn.attention.q_proj     -5.85573838396771 2.077788905399602\n",
      "transformer.h.6.attn.attention.out_proj   -4.390994106850984 2.008239952529349\n",
      "transformer.h.7.attn.attention.out_proj   -1.8989629396578165 2.7899494490972376\n",
      "transformer.h.7.attn.attention.v_proj     -1.9572087846151327 2.786851664868797\n",
      "transformer.h.7.ln_1                      -1.9572087846151327 2.786851664868797\n",
      "transformer.h.7.ln_2                      -0.15979023677547524 3.0089549000670273\n",
      "transformer.h.7.mlp.c_fc                  -0.18729266887759877 3.0076354683899305\n",
      "transformer.h.7.mlp.c_proj                -0.00038450520215365253 3.084142525021623\n",
      "transformer.wte                           18.196853951710025 4.541602184132833\n"
     ]
    }
   ],
   "source": [
    "for name, values in results.items():\n",
    "    x = np.array(ns)\n",
    "    y = np.array(values)\n",
    "    A = np.vstack([x, np.ones(len(x))]).T\n",
    "    k, b = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "    unexplained = np.square(y - (k * x + b)).mean() ** 0.5\n",
    "    if k > 2.0:\n",
    "        print(name, \" \" * (40 - len(name)), b, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\"2.\", \"3.\", \"4.\", \"5.\", \"6.\"]\n",
    "\n",
    "\n",
    "def low_rank_approximation(f: float = 1.0, d: float = 0.0) -> nn.Module:\n",
    "    new_model = AutoModelForCausalLM.from_pretrained(\"Mlxa/brackets-nested\")\n",
    "    for name, param in new_model.named_parameters():\n",
    "        if param.ndim != 2:\n",
    "            continue\n",
    "        U, S, Vh = t.linalg.svd(param.data, full_matrices=False)\n",
    "        pref = 0\n",
    "        while S[:pref].sum() < min(f * S.sum(), S.sum() - d):\n",
    "            pref += 1\n",
    "        S[pref:] = 0\n",
    "        print(name, pref, \"out of\", S.size(0))\n",
    "        param.data = U @ t.diag(S) @ Vh\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight 193 out of 256\n",
      "transformer.wpe.weight 171 out of 256\n",
      "transformer.h.0.attn.attention.k_proj.weight 88 out of 256\n",
      "transformer.h.0.attn.attention.v_proj.weight 39 out of 256\n",
      "transformer.h.0.attn.attention.q_proj.weight 101 out of 256\n",
      "transformer.h.0.attn.attention.out_proj.weight 58 out of 256\n",
      "transformer.h.0.mlp.c_fc.weight 160 out of 256\n",
      "transformer.h.0.mlp.c_proj.weight 144 out of 256\n",
      "transformer.h.1.attn.attention.k_proj.weight 62 out of 256\n",
      "transformer.h.1.attn.attention.v_proj.weight 82 out of 256\n",
      "transformer.h.1.attn.attention.q_proj.weight 62 out of 256\n",
      "transformer.h.1.attn.attention.out_proj.weight 95 out of 256\n",
      "transformer.h.1.mlp.c_fc.weight 170 out of 256\n",
      "transformer.h.1.mlp.c_proj.weight 136 out of 256\n",
      "transformer.h.2.attn.attention.k_proj.weight 35 out of 256\n",
      "transformer.h.2.attn.attention.v_proj.weight 75 out of 256\n",
      "transformer.h.2.attn.attention.q_proj.weight 34 out of 256\n",
      "transformer.h.2.attn.attention.out_proj.weight 80 out of 256\n",
      "transformer.h.2.mlp.c_fc.weight 178 out of 256\n",
      "transformer.h.2.mlp.c_proj.weight 166 out of 256\n",
      "transformer.h.3.attn.attention.k_proj.weight 32 out of 256\n",
      "transformer.h.3.attn.attention.v_proj.weight 59 out of 256\n",
      "transformer.h.3.attn.attention.q_proj.weight 32 out of 256\n",
      "transformer.h.3.attn.attention.out_proj.weight 57 out of 256\n",
      "transformer.h.3.mlp.c_fc.weight 115 out of 256\n",
      "transformer.h.3.mlp.c_proj.weight 86 out of 256\n",
      "transformer.h.4.attn.attention.k_proj.weight 15 out of 256\n",
      "transformer.h.4.attn.attention.v_proj.weight 35 out of 256\n",
      "transformer.h.4.attn.attention.q_proj.weight 14 out of 256\n",
      "transformer.h.4.attn.attention.out_proj.weight 41 out of 256\n",
      "transformer.h.4.mlp.c_fc.weight 98 out of 256\n",
      "transformer.h.4.mlp.c_proj.weight 73 out of 256\n",
      "transformer.h.5.attn.attention.k_proj.weight 11 out of 256\n",
      "transformer.h.5.attn.attention.v_proj.weight 28 out of 256\n",
      "transformer.h.5.attn.attention.q_proj.weight 9 out of 256\n",
      "transformer.h.5.attn.attention.out_proj.weight 37 out of 256\n",
      "transformer.h.5.mlp.c_fc.weight 105 out of 256\n",
      "transformer.h.5.mlp.c_proj.weight 76 out of 256\n",
      "transformer.h.6.attn.attention.k_proj.weight 44 out of 256\n",
      "transformer.h.6.attn.attention.v_proj.weight 76 out of 256\n",
      "transformer.h.6.attn.attention.q_proj.weight 38 out of 256\n",
      "transformer.h.6.attn.attention.out_proj.weight 66 out of 256\n",
      "transformer.h.6.mlp.c_fc.weight 110 out of 256\n",
      "transformer.h.6.mlp.c_proj.weight 89 out of 256\n",
      "transformer.h.7.attn.attention.k_proj.weight 55 out of 256\n",
      "transformer.h.7.attn.attention.v_proj.weight 79 out of 256\n",
      "transformer.h.7.attn.attention.q_proj.weight 67 out of 256\n",
      "transformer.h.7.attn.attention.out_proj.weight 100 out of 256\n",
      "transformer.h.7.mlp.c_fc.weight 188 out of 256\n",
      "transformer.h.7.mlp.c_proj.weight 176 out of 256\n",
      "4 14.718188285827637 12.1524658203125\n",
      "8 36.261539459228516 26.09669303894043\n",
      "16 94.45762634277344 57.6170654296875\n",
      "24 173.3188934326172 106.35701751708984\n",
      "32 243.24801635742188 163.08677673339844\n"
     ]
    }
   ],
   "source": [
    "lora = low_rank_approximation(f=1.0, d=60.0)\n",
    "for n in ns:\n",
    "    print(n, loss_metric(lora, tokenizer, n), baselines[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
