{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from beartype import beartype as typed\n",
    "from beartype.door import die_if_unbearable as assert_type\n",
    "from datasets import load_dataset\n",
    "from jaxtyping import Float, Int, Bool\n",
    "from typing import Callable\n",
    "from torch import Tensor as TT\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from einops import einops as ein\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class SparseAutoEncoder(nn.Module):\n",
    "    @typed\n",
    "    def __init__(self, in_features: int, h_features: int):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.h_features = h_features\n",
    "        self.weight = nn.Parameter(t.empty((in_features, h_features)))\n",
    "        self.bias = nn.Parameter(t.empty((h_features,)))\n",
    "        bound = (in_features * h_features) ** -0.25\n",
    "        t.nn.init.normal_(self.weight, -bound, bound)\n",
    "        t.nn.init.normal_(self.bias, -bound, bound)\n",
    "\n",
    "    @typed\n",
    "    def encode(self, x: Float[TT, \"... in_features\"]) -> Float[TT, \"... h_features\"]:\n",
    "        return F.relu(x @ self.weight + self.bias)\n",
    "\n",
    "    @typed\n",
    "    def decode(self, x: Float[TT, \"... h_features\"]) -> Float[TT, \"... in_features\"]:\n",
    "        return x @ self.weight.T\n",
    "\n",
    "    @typed\n",
    "    def forward(\n",
    "        self, x: Float[TT, \"... in_features\"]\n",
    "    ) -> tuple[Float[TT, \"... in_features\"], Float[TT, \"... h_features\"]]:\n",
    "        code = self.encode(x)\n",
    "        decoded = self.decode(code)\n",
    "        return decoded, code\n",
    "\n",
    "\n",
    "@typed\n",
    "def fit_sae(\n",
    "    input: Float[TT, \"total_tokens d\"],\n",
    "    hidden_dim: int,\n",
    "    lr: float,\n",
    "    l1: float = 0.0,\n",
    "    batch_size: int = 512,\n",
    "    epochs: int = 10,\n",
    ") -> SparseAutoEncoder:\n",
    "    model = SparseAutoEncoder(input.size(-1), hidden_dim)\n",
    "    optim = t.optim.Adam(model.parameters(), lr=lr)\n",
    "    dataloader = t.utils.data.DataLoader(\n",
    "        t.utils.data.TensorDataset(input),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    pbar = tqdm(range(epochs))\n",
    "    losses = []\n",
    "    for _ in pbar:\n",
    "        for x in dataloader:\n",
    "            optim.zero_grad()\n",
    "            p, z = model(x)\n",
    "            loss = F.mse_loss(p, x) + l1 * z.abs().mean()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            losses.append(loss.item())\n",
    "            half_size = (len(losses) + 1) // 2\n",
    "            second_half_mean = sum(losses[-half_size:]) / half_size\n",
    "            pbar.set_postfix_str(f\": {second_half_mean:.3f}\")\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
